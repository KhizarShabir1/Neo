{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 730
    },
    "colab_type": "code",
    "id": "sNMGKBkS3weV",
    "outputId": "42aff759-b082-4175-f5b1-2731e50e2830"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.4)\n",
      "Collecting pytorch-ignite\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/31/efcc2b587419b1f54c5c6ef51996f91bb5d8f760537d17de674c89e06048/pytorch_ignite-0.2.1-py2.py3-none-any.whl (84kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 3.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-ignite) (1.3.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-ignite) (1.17.4)\n",
      "Installing collected packages: pytorch-ignite\n",
      "Successfully installed pytorch-ignite-0.2.1\n",
      "Collecting tensorboardX==1.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)\n",
      "\u001b[K     |████████████████████████████████| 225kB 3.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.8) (1.17.4)\n",
      "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.8) (3.10.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.8) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX==1.8) (42.0.2)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-1.8\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.17.4)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.6)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (42.0.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install pytorch-ignite\n",
    "!pip install pytorch-transformers>=1.2\n",
    "!pip install tensorboardX==1.8\n",
    "!pip install tensorflow  # for tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "kDFBhO1T4THA",
    "outputId": "4089a7f4-ca7f-4b58-f936-488fc7f0ea10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F3qbYw2Z4ppt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import logging\n",
    "from pprint import pformat\n",
    "from collections import defaultdict\n",
    "from argparse import ArgumentParser\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import Accuracy, Loss, MetricsLambda, RunningAverage\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.contrib.handlers import ProgressBar, PiecewiseLinear\n",
    "\n",
    "from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger, OutputHandler, OptimizerParamsHandler\n",
    "from pytorch_transformers import (AdamW, OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer,\n",
    "                                  GPT2DoubleHeadsModel, GPT2Tokenizer, WEIGHTS_NAME, CONFIG_NAME)\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "import socket\n",
    "import tarfile\n",
    "from pytorch_transformers import cached_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75d5w6hh8F5o"
   },
   "outputs": [],
   "source": [
    "HF_FINETUNED_MODEL = \"https://s3.amazonaws.com/models.huggingface.co/transfer-learning-chatbot/gpt_personachat_cache.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "APLQCThS64KM"
   },
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--dataset_cache\", type=str, default='./dataset_cache', help=\"-->Path or url of the dataset cache\")\n",
    "parser.add_argument(\"--model_checkpoint\", type=str, default=\"openai-gpt\", help=\"-->Path, url or short name of the model\")\n",
    "parser.add_argument(\"--mc_coef\", type=float, default=1.0, help=\"-->Multiple-choice loss coef\")\n",
    "parser.add_argument(\"--max_norm\", type=float, default=1.0, help=\"-->gradient norm (clipping)\")\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=1, help=\"-->Number of training epochs\")\n",
    "parser.add_argument(\"--train_batch_size\", type=int, default=4, help=\"-->training batch size \")\n",
    "parser.add_argument(\"--valid_batch_size\", type=int, default=4, help=\"-->validation batch size\")\n",
    "parser.add_argument(\"--num_candidates\", type=int, default=2, help=\"-->no of candidates for training\")\n",
    "parser.add_argument(\"--max_history\", type=int, default=2, help=\"-->previous dialogue no to keep in history\")\n",
    "parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=8, help=\"-->Gradients accumulating on several steps\")\n",
    "parser.add_argument(\"--lr\", type=float, default=6.25e-5, help=\"-->Learning rate\")\n",
    "parser.add_argument(\"--lm_coef\", type=float, default=1.0, help=\"-->LM loss coef\")\n",
    "parser.add_argument(\"--eval_before_start\", action='store_true', help=\"-->If true start with a first evaluation before training\")\n",
    "parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"-->selection of device for training -GPU or CPU-\")\n",
    "parser.add_argument(\"--fp16\", type=str, default=\"\", help=\"-->Set to O0, O1, O2 or O3 for fp16 training (see apex documentation)\")\n",
    "parser.add_argument(\"--personality_permutations\", type=int, default=1, help=\"-->setting permutation no of personality Sentences\")\n",
    "\n",
    "parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"-->Local rank for distributed training (-1: not distributed)\")\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jeqkbosRkd9S"
   },
   "outputs": [],
   "source": [
    "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
    "                         'additional_special_tokens': ('<speaker1>', '<speaker2>')}\n",
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
    "MODEL_INPUTS = [\"input_ids\", \"mc_token_ids\", \"lm_labels\", \"mc_labels\", \"token_type_ids\"]\n",
    "PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]\n",
    "def addingSpecialTokens_(model, tokenizer):\n",
    "\n",
    "    prevTokens = len(tokenizer.encoder)\n",
    "    newTokensLen = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN) \n",
    "    # making sure new tokens are being are being added\n",
    "    if newTokensLen > 0: \n",
    "        model.resize_token_embeddings(new_num_tokens= newTokensLen + prevTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "Sf3_PxiFFmex",
    "outputId": "af23fa4d-f48f-4833-f22d-650909a95167"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 815973/815973 [00:00<00:00, 5447817.52B/s]\n",
      "100%|██████████| 458495/458495 [00:00<00:00, 3810642.50B/s]\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "100%|██████████| 273/273 [00:00<00:00, 56551.02B/s]\n",
      "100%|██████████| 478750579/478750579 [00:11<00:00, 41328402.92B/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if \"gpt2\" in args.model_checkpoint:\n",
    "    tokenizer_class = GPT2Tokenizer\n",
    "else:\n",
    "    tokenizer_class = OpenAIGPTTokenizer\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(args.model_checkpoint)\n",
    "\n",
    "if \"gpt2\" in args.model_checkpoint:\n",
    "    model_class = GPT2DoubleHeadsModel\n",
    "else:\n",
    "    model_class = OpenAIGPTDoubleHeadsModel\n",
    "model = model_class.from_pretrained(args.model_checkpoint)\n",
    "model.to(args.device)\n",
    "addingSpecialTokens_(model, tokenizer) # adding tokens\n",
    "optimizer = AdamW(model.parameters(), lr=args.lr, correct_bias=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAbcCXUlxIeG"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#                                                           Code for reading file on colab\n",
    "\n",
    "# !pip install -U -q PyDrive\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "# # Authenticate and create the PyDrive client.\n",
    "# auth.authenticate_user()\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = GoogleCredentials.get_application_default()\n",
    "# drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "4VxYr2MY2h2T",
    "outputId": "abcb6e53-dbb7-4a05-c42f-f53271a1ab32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "os.chdir(\"/content/drive/My Drive/fyp1\")\n",
    "# with open('dataset.json') as json_file:\n",
    "#     itemData = json.load(json_file)\n",
    "# with open('dataset.json', \"r\", encoding=\"utf-8\") as f:\n",
    "#     dataset = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P4dlaF74lcdc"
   },
   "outputs": [],
   "source": [
    "with open('small_dataset.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H7OePu-V413l"
   },
   "outputs": [],
   "source": [
    "def tokenize(obj): # three types of inputs are acceptable string dictonary and list\n",
    "    if isinstance(obj, str):\n",
    "        return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
    "    if isinstance(obj, dict):\n",
    "        return dict((n, tokenize(o)) for n, o in obj.items())\n",
    "    return list(tokenize(o) for o in obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1KDMHJ094Qco"
   },
   "outputs": [],
   "source": [
    "dataset_cache=args.dataset_cache\n",
    "dataset_cache = dataset_cache + '_' + type(tokenizer).__name__\n",
    "dataset = tokenize(dataset)\n",
    "torch.save(dataset, dataset_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ps0UfpNyFnbc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8b_t4ZR9FnXX"
   },
   "outputs": [],
   "source": [
    "def build_input_from_segments(persona, history, reply, tokenizer, lm_labels=False, with_eos=True):\n",
    "    ##Build a sequence of input from 3 segments: persona, history and last reply.\n",
    "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])]\n",
    "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence))\n",
    "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
    "    instance[\"lm_labels\"] = [-1] * len(instance[\"input_ids\"])\n",
    "    if lm_labels:\n",
    "        instance[\"lm_labels\"] = ([-1] * sum(len(s) for s in sequence[:-1])) + [-1] + sequence[-1][1:]\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ON94qHMmJU-W"
   },
   "outputs": [],
   "source": [
    "def pad_dataset(dataset, padding=0):\n",
    "    #Pad the dataset. This could be optimized by defining a Dataset class and padding at the batch level, but this is simpler. \n",
    "    k=0\n",
    "    for x in dataset[\"input_ids\"]:\n",
    "        if len(x)>k:\n",
    "            k=len(x)\n",
    "    # max_l = max(len(x) for x in dataset[\"input_ids\"])\n",
    "\n",
    "    for name in PADDED_INPUTS:\n",
    "        dataset[name] = [x + [padding if name != \"lm_labels\" else -1] * (k - len(x)) for x in dataset[name]]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uaahl9BHPqHp"
   },
   "outputs": [],
   "source": [
    "def average_distributed_scalar(scalar, args):\n",
    "    \"\"\" Average a scalar over the nodes if we are in distributed training. We use this for distributed evaluation. \"\"\"\n",
    "    if args.local_rank == -1:\n",
    "        return scalar\n",
    "    scalar_t = torch.tensor(scalar, dtype=torch.float, device=args.device) / torch.distributed.get_world_size()\n",
    "    torch.distributed.all_reduce(scalar_t, op=torch.distributed.ReduceOp.SUM)\n",
    "    return scalar_t.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-xUbs3iQZ9FX"
   },
   "outputs": [],
   "source": [
    "def get_data_loaders(dataset_, args, tokenizer):\n",
    "    \"\"\" Prepare the dataset for training and evaluation \"\"\"\n",
    "    originalDataset = dataset_\n",
    "\n",
    "  \n",
    "    datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n",
    "    for dataset_name, dataset in originalDataset.items():\n",
    "        num_candidates = len(dataset[0][\"utterances\"][0][\"candidates\"])\n",
    "        if args.num_candidates > 0 and dataset_name == 'train':\n",
    "            num_candidates = min(args.num_candidates, num_candidates)\n",
    "        for dialog in dataset:\n",
    "            persona = dialog[\"personality\"].copy()\n",
    "            for _ in range(args.personality_permutations):\n",
    "                for utterance in dialog[\"utterances\"]:\n",
    "                    history = utterance[\"history\"][-(2*args.max_history+1):]\n",
    "                    for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\n",
    "                        lm_labels = bool(j == num_candidates-1)\n",
    "                        instance = build_input_from_segments(persona, history, candidate, tokenizer, lm_labels)\n",
    "                        for input_name, input_array in instance.items():\n",
    "                            datasets[dataset_name][input_name].append(input_array)\n",
    "                    datasets[dataset_name][\"mc_labels\"].append(num_candidates - 1)\n",
    "                    datasets[dataset_name][\"n_candidates\"] = num_candidates\n",
    "                persona = [persona[-1]] + persona[:-1]  # permuted personalities\n",
    "\n",
    "   \n",
    "    tensor_datasets = {\"train\": [], \"valid\": []}\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        dataset = pad_dataset(dataset, padding=tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n",
    "        for input_name in MODEL_INPUTS:\n",
    "            tensor = torch.tensor(dataset[input_name])\n",
    "            if input_name != \"mc_labels\":\n",
    "                tensor = tensor.view((-1, datasets[dataset_name][\"n_candidates\"]) + tensor.shape[1:])\n",
    "            tensor_datasets[dataset_name].append(tensor)\n",
    "\n",
    "\n",
    "    train_dataset, valid_dataset = TensorDataset(*tensor_datasets[\"train\"]), TensorDataset(*tensor_datasets[\"valid\"])\n",
    "    train_sampler =  None\n",
    "    valid_sampler =  None\n",
    "    train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, shuffle=(True))\n",
    "    valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=args.valid_batch_size, shuffle=False)\n",
    "\n",
    "    # logger.info(\"Train dataset (Batch, Candidates, Seq length): {}\".format(train_dataset.tensors[0].shape))\n",
    "    # logger.info(\"Valid dataset (Batch, Candidates, Seq length): {}\".format(valid_dataset.tensors[0].shape))\n",
    "    return train_loader, valid_loader, train_sampler, valid_sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LvfKE1R6BdZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pmQb01f4MUXj"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_loader, val_loader, train_sampler, valid_sampler = get_data_loaders(dataset, args, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "876NnTrntu9j"
   },
   "outputs": [],
   "source": [
    "def update(engine, batch):\n",
    "    model.train()\n",
    "    batch = tuple(input_tensor.to(args.device) for input_tensor in batch)\n",
    "    input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
    "    (lm_loss), (mc_loss), *_ = model(\n",
    "        input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,\n",
    "        mc_labels=mc_labels, lm_labels=lm_labels\n",
    "    )\n",
    "    loss = (lm_loss * args.lm_coef + mc_loss * args.mc_coef) / args.gradient_accumulation_steps\n",
    "    if args.fp16:\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_norm)\n",
    "    else:\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_norm)\n",
    "    if engine.state.iteration % args.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return loss.item()\n",
    "trainer = Engine(update)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qc3A3YT4MR7U"
   },
   "outputs": [],
   "source": [
    "# Evaluation function and evaluator (evaluator output is the input of the metrics)\n",
    "def inference(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch = tuple(input_tensor.to(args.device) for input_tensor in batch)\n",
    "        input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
    "        # logger.info(tokenizer.decode(input_ids[0, -1, :].tolist()))\n",
    "        # if we dont send labels to model, it doesnt return losses\n",
    "        lm_logits, mc_logits, *_ = model(\n",
    "            input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,\n",
    "        )\n",
    "        lm_logits_flat_shifted = lm_logits[..., :-1, :].contiguous().view(-1, lm_logits.size(-1))\n",
    "        lm_labels_flat_shifted = lm_labels[..., 1:].contiguous().view(-1)\n",
    "        return (lm_logits_flat_shifted, mc_logits), (lm_labels_flat_shifted, mc_labels)\n",
    "evaluator = Engine(inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrdbKxtY_Szz"
   },
   "outputs": [],
   "source": [
    "# Attach evaluation to trainer: we evaluate when we start the training and at the end of each epoch\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, lambda _: evaluator.run(val_loader))\n",
    "if args.n_epochs < 1:\n",
    "    trainer.add_event_handler(Events.COMPLETED, lambda _: evaluator.run(val_loader))\n",
    "if args.eval_before_start:\n",
    "    trainer.add_event_handler(Events.STARTED, lambda _: evaluator.run(val_loader))\n",
    "\n",
    "\n",
    "\n",
    "# Linearly decrease the learning rate from lr to zero\n",
    "scheduler = PiecewiseLinear(optimizer, \"lr\", [(0, args.lr), (args.n_epochs * len(train_loader), 0.0)])\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
    "\n",
    "# Prepare metrics - note how we compute distributed metrics\n",
    "RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")\n",
    "metrics = {\"nll\": Loss(torch.nn.CrossEntropyLoss(ignore_index=-1), output_transform=lambda x: (x[0][0], x[1][0])),\n",
    "            \"accuracy\": Accuracy(output_transform=lambda x: (x[0][1], x[1][1]))}\n",
    "metrics.update({\"average_nll\": MetricsLambda(average_distributed_scalar, metrics[\"nll\"], args),\n",
    "                \"average_accuracy\": MetricsLambda(average_distributed_scalar, metrics[\"accuracy\"], args)})\n",
    "metrics[\"average_ppl\"] = MetricsLambda(math.exp, metrics[\"average_nll\"])\n",
    "for name, metric in metrics.items():\n",
    "    metric.attach(evaluator, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jOOL5zvmb7N0"
   },
   "outputs": [],
   "source": [
    "def make_logdir(model_name: str):\n",
    "    \"\"\"Create unique path to save results and checkpoints, e.g. runs/Sep22_19-45-59_gpu-7_gpt2\"\"\"\n",
    "    # Code copied from ignite repo\n",
    "    current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "    logdir = os.path.join(\n",
    "        'runs', current_time + '_' + socket.gethostname() + '_' + model_name)\n",
    "    return logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2d4ce6ae12ed43d1a4d64133fb08f53c",
      "c953d9f0d11f4202a1eb93aacc618898",
      "a41a365184d64ae99817495516133a77",
      "e0b8ae9bb9aa47948b210b74c555ab97",
      "94704202938c4610b31f1537d569be53",
      "e535b5325bc04341b202fd25b2d19731",
      "210479f0bbbe41d7bdec9eb69c531aac",
      "9f7bc384913746d3b90c2f6f884f0670"
     ]
    },
    "colab_type": "code",
    "id": "7fDmt5ngkQEm",
    "outputId": "fa572918-82f7-4761-907f-5a253195d565"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4ce6ae12ed43d1a4d64133fb08f53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=91), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-22-dfd6071daa25>\", line 21, in <module>\n",
      "    trainer.run(train_loader, max_epochs=2)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 446, in run\n",
      "    self._handle_exception(e)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 410, in _handle_exception\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 437, in run\n",
      "    self._fire_event(Events.EPOCH_COMPLETED)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 345, in _fire_event\n",
      "    func(self, *(event_args + args), **kwargs)\n",
      "  File \"<ipython-input-20-e30318fd2e18>\", line 1, in <lambda>\n",
      "    trainer.add_event_handler(Events.EPOCH_COMPLETED, lambda _: evaluator.run(val_loader))\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 446, in run\n",
      "    self._handle_exception(e)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 410, in _handle_exception\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 433, in run\n",
      "    hours, mins, secs = self._run_once_on_dataset()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 399, in _run_once_on_dataset\n",
      "    self._handle_exception(e)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 410, in _handle_exception\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 391, in _run_once_on_dataset\n",
      "    self.state.output = self._process_function(self, batch)\n",
      "  File \"<ipython-input-19-cf30ccd81e9a>\", line 9, in inference\n",
      "    input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_transformers/modeling_openai.py\", line 693, in forward\n",
      "    head_mask=head_mask)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_transformers/modeling_openai.py\", line 518, in forward\n",
      "    outputs = block(hidden_states, head_mask[i])\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_transformers/modeling_openai.py\", line 354, in forward\n",
      "    m = self.mlp(n)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_transformers/modeling_openai.py\", line 335, in forward\n",
      "    h = self.act(self.c_fc(x))\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_transformers/modeling_utils.py\", line 623, in forward\n",
      "    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/usr/lib/python3.6/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# On the main process: add progress bar, tensorboard, checkpoints and save model, configuration and tokenizer before we start to train\n",
    "if args.local_rank in [-1, 0]:\n",
    "    pbar = ProgressBar(persist=True)\n",
    "    pbar.attach(trainer, metric_names=[\"loss\"])\n",
    "    evaluator.add_event_handler(Events.COMPLETED, lambda _: pbar.log_message(\"Validation: %s\" % pformat(evaluator.state.metrics)))\n",
    "\n",
    "    log_dir = make_logdir(args.model_checkpoint)\n",
    "    tb_logger = TensorboardLogger(log_dir)\n",
    "\n",
    "    tb_logger.attach(trainer, log_handler=OutputHandler(tag=\"training\", metric_names=[\"loss\"]), event_name=Events.ITERATION_COMPLETED)\n",
    "    tb_logger.attach(trainer, log_handler=OptimizerParamsHandler(optimizer), event_name=Events.ITERATION_STARTED)\n",
    "    tb_logger.attach(evaluator, log_handler=OutputHandler(tag=\"validation\", metric_names=list(metrics.keys()), another_engine=trainer), event_name=Events.EPOCH_COMPLETED)\n",
    "\n",
    "    checkpoint_handler = ModelCheckpoint(log_dir, 'checkpoint', save_interval=1, n_saved=3)\n",
    "    trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'mymodel': getattr(model, 'module', model)})  # \"getattr\" takes care of distributed encapsulation\n",
    "\n",
    "    torch.save(args, log_dir + '/model_training_args.bin')\n",
    "    getattr(model, 'module', model).config.to_json_file(os.path.join(log_dir, CONFIG_NAME))\n",
    "    tokenizer.save_pretrained(log_dir)\n",
    "\n",
    "# Run the training\n",
    "trainer.run(train_loader, max_epochs=2)\n",
    "\n",
    "# On the main process: close tensorboard logger and rename the last checkpoint (for easy re-loading with OpenAIGPTModel.from_pretrained method)\n",
    "if args.local_rank in [-1, 0] and args.n_epochs > 0:\n",
    "    os.rename(checkpoint_handler._saved[-1][1][-1], os.path.join(log_dir, WEIGHTS_NAME))  # TODO: PR in ignite to have better access to saved file paths (cleaner)\n",
    "    tb_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s7emkiIPKlEI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "fyp1Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "210479f0bbbe41d7bdec9eb69c531aac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2d4ce6ae12ed43d1a4d64133fb08f53c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a41a365184d64ae99817495516133a77",
       "IPY_MODEL_e0b8ae9bb9aa47948b210b74c555ab97"
      ],
      "layout": "IPY_MODEL_c953d9f0d11f4202a1eb93aacc618898"
     }
    },
    "94704202938c4610b31f1537d569be53": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9f7bc384913746d3b90c2f6f884f0670": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a41a365184d64ae99817495516133a77": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "Epoch [1/2]",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e535b5325bc04341b202fd25b2d19731",
      "max": 91,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_94704202938c4610b31f1537d569be53",
      "value": 0
     }
    },
    "c953d9f0d11f4202a1eb93aacc618898": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0b8ae9bb9aa47948b210b74c555ab97": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f7bc384913746d3b90c2f6f884f0670",
      "placeholder": "​",
      "style": "IPY_MODEL_210479f0bbbe41d7bdec9eb69c531aac",
      "value": "[91/91] 100%|██████████, loss=0.626 [18:08&lt;00:00]"
     }
    },
    "e535b5325bc04341b202fd25b2d19731": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
